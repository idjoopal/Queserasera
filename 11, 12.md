# 11. 심층 신경망 훈련하기

## 11.4 규제를 사용해 과대적합 피하기

- 과대적합을 피하기 위해 적정한 규제를 통해 모델의 자유도를 제한

### 11.4.1 l1과 l2 규제

- 신경망의 연결 가중치를 제한하기 위해 l2 규제를 하거나 (대부분의 가중치가 0인) 희소모델을 만들기 위해 l1 규제를 사용
- l2 함수는 훈련 중 규제 손실을 계산하기 위해 각 스텝에서 호출되는 규제 객체를 반환하며 이 손실을 최종 손실에 합산한다.

### 11.4.2 드롭아웃

- 정확도를 더 높여줄 수 있는 규제 기법
- 각 훈련 스텝에서 뉴런이 임시로 Dropout 될 확률 p를 가지는 구조
- 훈련에만 드롭아웃을 적용하며 훈련이 종료되면 적용하지 않음
- ![img](https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMDAxMzBfODMg%2FMDAxNTgwMzY3ODA5NTc2.xCPG4sEmnY8N23NFKboZCj-UTh-RUB932xvvXWucrWgg.jROjqV-OtMVi4h6tVMYKZHb9lZOaDxPAlXEXORUWRngg.PNG.kkang9901%2Fimage.png&type=sc960_832)

- RNN에선 약 20~30%, CNN에서는 40~50%의 드롭아웃률을 가진다.
- 입력값의 작은변화에 덜 민감해져 네트워크가 안정적으로 바뀌고 일반화 성능이 좋아진다.
- 훈련에 줄어든 드롭아웃(50%라면 각 노드는 입력되는 값이 절반정도다.)만큼 테스트 시에 보정치를 추가해야한다.
  - 각 입력의 연결 가중치에 보존 확률(1-p)를 곱해준다.
- 모델이 과대적합하면 드롭아웃을 늘리고, 과소적합하면 드롭아웃을 줄인다.

### 11.4.3 몬테 카를로 드롭아웃

- 앞서 훈련한 드롭아웃 모델을 수정하지 않고 성능을 늘리는 방법.
- 모델 훈련뿐만 아니라, 모델 예측에도 드롭아웃을 사용한다.
  - 기존 모델의 경우 예측에만 추가로 사용하면 된다.
  - 테스트에 대해 서로 다른 n번의 예측을 수행하여 평균을 구한다.
  - 이렇게 구해진 평균은 예측과 불확실성에 대한 추정이 더 정확해진다.

### 11.4.4 맥스-노름(norm) 규제

- 각각의 뉴런에 대해 입력의 연결 가중치 w가 ||w||_2 <= r 이 되도록 제한한다.
- r은 맥스-노름 하이퍼파라미터, ||w||_2는 l2 노름을 나타낸다.
- 맥스-노름 규제는 전체 손실 함수에 규제 손실 항을 추가하지 않는다. 대신 훈련 스텝마다 ||w||_2 를 계산하고 필요할 때 w의 스케일을 조정한다. (w<- w*r/||w||2)
- 불안정한 그레이디언트 문제를 완화하는데 도움을 줄 수 있다.
  

## 11.5 요약 및 실용적인 가이드라인

- 기본설정
  - 커널 초기화 : He 초기화
  - 활성화 함수 : ELU
  - 정규화 : 깊을 경우에만 배치 정규화
  - 규제 : 조기종료
  - 옵티마이저 : 모멘텀 최적화
  - 학습률 스케줄 : 1사이클
- 자기 정규화를 위한 DNN
  - 커널 초기화 : 르쿤 초기화
  - 활성화 함수 : SELU
  - 정규화 : 없음
  - 규제 : 필요하다면 알파 드롭아웃(SELU를 사용할 때 정규화가 깨지는 문제를 해결해준다.)
  - 옵티마이저 : 모멘텀 최적화
  - 학습률 스케줄 : 1사이클
    

# 12. 텐서플로를 사용한 사용자 정의 모델과 훈련

## 12.1 텐서플로 훑어보기

- 텐서플로가 제공하는 것
  - 넘파이와 비슷하지만 GPU를 지원
  - 분산 컴퓨팅을 지원
  - JustInTime 컴파일러를 포함하며 계산을 최적화함
  - 플랫폼에 중립적인 포맷으로 모델을 내보낼 수 있음
  - 자동 미분 및 고성능 옵티마이저를 제공

- 실습 부분 생략