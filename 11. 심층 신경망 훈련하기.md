https://blog.naver.com/jesusss91/221617701126

# 11. 심층 신경망 훈련하기

## 11.1 그레이디언트 소실과 폭주 문제

- 그레이디언트 소실 : 역전파 알고리즘에서 계산이 하위층으로 진행될수록 그레이디언트가 점점 작아지는 경우 발생
- 그레이디언트 폭주 : 그레이디언트가 점점 커져서 여러층이 비정상적으로 큰 가중치로 갱신
- 주요 원인은 로지스틱 활성화 함수이며 입력이 커지면 0, 1로 수렴하면서 기울기가 0에 가까워지는 문제가 있다.

### 11.1.1 글로럿과 He 초기화

- 2010년에 발표한 글로럿과 벤지오의 논문을 통해 불안정한 그레이디언트 문제를 완화하는 방법을 제안함
  - 신호가 소멸, 폭주하지 않으려면 각 층의 출력에 대한 분산과 입력에 대한 분산이 같아야 한다고 주장
  - 역방향에서 층을 통과하기 전 후의 그레이디언트 분산이 동일해야한다.
- 세이비어 초기화, 글로럿 초기화
  - 평균이 0이고 표준편차가 Var(W) 를 만족한다.
    -  ![img](https://blog.kakaocdn.net/dn/qwFcz/btqCy3AzLR9/HLCL3fr3j223kK1FNPgdy0/img.png)nin+nout = navg*2
  - 각 층의 연결 가중치를 무작위로 초기화 하는 방법 사용
  - 여러 층의 기울기 분산의 균형을 유지해준다.
  - Sigmoid와 Tanh 함수에는 성능이 좋지만 ReLU에 사용하면 층이 깊어질 수록 0으로 치우쳐진다.
- 르쿤 초기화
  - navg대신 nin을 사용한 것.
- He 초기화
  - ReLU 입력이 음수일 때 출력이 0이므로 이를 고르게 분포시키기 위해 사용함.
- 비교
  - 글로럿
    - 활성화 함수 없음, Tanh, Logistic, Softmax
    - 1/fan_avg
  - He
    - ReLU 함수와 변종
    - 2/fan_in
  - 르쿤
    - SELU
    - 1/fan_in

### 11.1.2 수렴하지 않는 활성화 함수

- ReLU가 계산이 빠르지만 완벽하지 않으며 죽은 ReLU문제를 가지고 있다.
  - 일부 뉴런이 0 이외의 값을 출력하지 않음
- 문제를 해결하기 위해 LeakyReLU와 같은 변종을 사용한다.
  - ![img](https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMTAzMTFfOTAg%2FMDAxNjE1NDU4MzAwNDE5.pwbGa_wAjIxuxcHo-ZCdrnJVBFVJb8p6dU0b4IB6_Dwg.qApyhMSGvt0ANnmiLmbqztexiD79kCwjbVyYmd2VtkEg.PNG.mmmy2513%2Fimage.png&type=sc960_832)
  - 음수부분이 작은 기울기를 가져, Dead ReLU문제가 발생하지 않으며, 혼수상태가 이어져도 추후 뉴런이 다시 깨어날 수 있다.
- RReLU
  - 하이퍼파라미터 alpha를  무작위로 선택하고 테스트시에 평균을 사용함
- PReLU
  - alpha를 하이퍼파라미터로 쓰지않고 훈련하는동안 역전파에 의해 변경하며 학습하는 방식
- ELU라는 활성화 함수를 통해 ReLU의 다양한 변종보다 성능을 좋게 할 수 있다.
  - ![img](https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMTAzMTFfNDEg%2FMDAxNjE1NDU4NDk4MjMz.9f-XIG0IQNQObumzdgzGUQwlMn1LhgX7-tnvp4UephIg._dj9B7XmRBwGsKqmyBqt_Yjnvt2Q6A-ZqCOAoaaxDhgg.PNG.mmmy2513%2Fimage.png&type=sc960_832)
  - 입력값이 음수여도 그레이디언트가 0이 아니므로 죽은 뉴런이 발생하지 않음.
  - 음수의 영역이 지수함수 값이여서 계산이 다소 느리다는 단점이 있다.
- SELU
  - 스케일이 조정된 ELU
  - 각 층의 출력이 평균 0과 표준편차 1을 유지하며, 입력값 역시 표준화 되어 있어야 한다.
  - 일렬로 된 완전 연결 층에서만 표준화를 보장한다.
- 심층 신경망의 은닉층에 사용할 활성화 함수
  - SELU > ELU > LeakyReLU > ReLU > tanh > 로지스틱
  - 대부분의 라이브러리와 HW 가속기 들이 ReLU에 최적화 되어있음.

### 11.1.3 배치 정규화

- 그레이디언트 폭주와 소실을 해결하기 위해 배치 정규화 기법을 제안(이오페, 세게지)
- 활성화 함수 통과 전이나 후에 연산을 추가하여 결과값의 스케일을 조정하고 이동시킴.
- 그레이디언트 소실문제가 감소하여 tanh이나 로지스틱같은 함수를 사용할 수 있다
- 가중치 초기화에 덜 민감해져서 큰 학습률로 학습속도를 높일 수 있다.
- 규제 역할을 해주는 장점
- 모델의 복잡도를 높이며 실행시간이 늘어나는 단점
  - 층마다 계산이 추가되기 때문
  - 에포크별 계산은 오래걸리지만 그만큼 수렴이 빨라져 상쇄되며 실제 걸리는 시간은 더 짧은 편.

### 11.1.4 그레이디언트 클리핑

- 그레이디언트 폭주문제를 완화하는 방법
- 일정 임계값을 넘어서는 그레이디언트를 제거하는 것.
- 배치 정규화로 충분하지만 이를 적용하기 어려운 신경망에 사용함.



## 11.2 사전훈련된 층 재사용하기

- 전이학습 : 큰 DNN은 새로 훈련하는 것보다, 비슷한 문제를 푼 신경망의 하위층을 재사용하는 것이 좋다.
- 입력층과 가까운 하위층부터 재사용하며 작업이 비슷할 수록 그 수를 늘려나간다.
- 전이학습은 작은 완전 연결 네트워크에서는 잘 동작하지 않는다.  CNN에서 잘 동작하는 경향.



### 11.2.2 비지도 사전훈련

- 레이블이 없는 데이터를 얻는건 싸지만, 여기에 레이블을 다는 것은 비싸다.
- 오토인코더나 GAN을 이용하여 레이블을 다는 방법을 사용한다.
- 초창기엔 제한된 볼츠만 머신(RBM)을 사용했으며, 은닉층을 하나씩 추가하며 비지도 학습모델을 훈련하여 마지막에 본래 훈련을 진행



### 11.2.3 보조 작업에서 사전훈련

- 레이블 된 데이터를 쉽게 얻거나 생성할 수 있는 보조 작업에서 첫번째 신경망을 훈련하는 것.
  - 이 신경망의 하위층을 재사용한다.



## 11.3 고속 옵티마이저

### 11.3.1 모멘텀 최적화

- 경사하강법은 일정한 속도로 내려가지만, 이전 그레이디언트와 비교하여 종단속도를 결정하고 평편한 지역에서의 경사하강의 속도를 빠르게 하는 것.

- 매 반복에서 현재 그레이디언트를 모멘텀 벡터 m에 더하고 이 값을 빼는 방식으로 가중치 theta를 갱신

- ![img](https://postfiles.pstatic.net/MjAxOTA4MTVfMTI4/MDAxNTY1ODc3Mjc0NDI2.2w6VR8mPNuF-64pVbVC6a4ygpKYmc0wMjlM8nJHAHDEg.cUY4dgGXb0dpv8JOENt-w2SuuKlmwQ6x4xS-bNQUgLIg.PNG.jesusss91/image.png?type=w966)

- 
  $$
  m ← \beta m - (lr)*gradient
  $$
  
  $$
  \theta ← \theta + m
  $$

- 파라미터 beta는 모멘텀이 너무 커지는것을 막기 위함이며 0~1사이 값을 사용. 보통 0.9를 사용한다.

- 그래디언트가 일정하다면 beta가 0.9일때 학습속도가 경사하강법보다 10배 빠르다.

- 마찰저항이 적으면(beta가 0.9이상이면), 최적값에 안정되기 전까지 최적값을 건너뛰었다가 돌아오는 식으로 근처에서 왔다 갔다 하는 경향이 있다.



### 11.3.2 네스테로프 가속 경사

-  모멘텀 최적화의 변종이며 기존보다 항상 더 빠름
- 현재 위치가 theta가 아니라 모멘텀의 방향으로 조금 앞선 theta + beta m 에서 비용 함수의 그라디언트를 계산함.
- ![img](https://postfiles.pstatic.net/MjAxOTA4MTVfMjU4/MDAxNTY1ODc4NTQ1NDY5.xL39GiRwx7i9k09OBM8LEu_IaRz4VhKQgexWgAdu_Ggg.9NshHYKQ62N-cPMa099RvfO_IvpFUe1b-5O_CSZN2NMg.PNG.jesusss91/image.png?type=w966)
- ![img](https://postfiles.pstatic.net/MjAxOTA4MTVfMTQz/MDAxNTY1ODc4NDUyMTY3.xqpN6qxszD0k-SAIAZSJwz-eLxjltD7eXHKubiCTj8Ig.j1vhIDRe4rZ2W4C-a-XIY69aKKHrIwXtt1Xiy4S4OYIg.PNG.jesusss91/image.png?type=w966)
- 어느정도 최적값에 더 가까우며, 이 것이 쌓이면 확연히 빨라지게 된다.



### 11.3.3 AdaGrad

- 경사하강법은 우선 가파르게 하강한 다음, 최적점에 가까워 질수록 완만해짐
- AdaGrad는 처음부터 최적점을 향해 나아가도록 함.
- 각 가중치는 각자 다른 학습률을 가지고 움직여야한다는 생각.
- ![img](https://postfiles.pstatic.net/MjAxOTA4MTVfMTM5/MDAxNTY1ODc5NDk4MDUx.YVbFDTluhK5-rpSQbz-KNHbyY1xWzeZOJCM3SWRuXz8g.brxfNl0gvYWKa3cXs3vEkXLHSMnzcB-C-Lnt0cw2xgAg.PNG.jesusss91/image.png?type=w966)
- 원소별 곱셈을 통해 그레디언트를 누적시켜 제곱, 원소별 나눗셈으로 학습률을 비율적으로 조정.
- 학습률을 감소 시키며, 경사가 완만한 차원보다 가파른 차원에 대해 더 빠르게 감소된다.
- 신경망을 학습할 때 학습률이 너무 빨리 낮아져 학습이 멈춰버리는 문제가 있다. 그러므로 심층 신경망에는 사용하지 않아야 함.



### 11.3.4 RMSProp

- 모든 그레이디언트가 아닌 가장 최근 반복에서 비롯된 그레이디언트만 누적하여 AdaGrad의 문제를 해결함
- ![img](https://postfiles.pstatic.net/MjAxOTA4MTVfNzIg/MDAxNTY1ODc5NzE2ODE5.5fgBMhHx9TIp_hKtnOEz3D-ZZLcKxAeR08JyoaSF4aIg.-wzVzD2-OL9qos6IxWv2TBzbuIjUb-KvulggkcQvVK4g.PNG.jesusss91/image.png?type=w966)
- 추가된 파라미터는 기본값 0.9로 둬도 잘 돌아간다.



### 11.3.5 Adam과 Nadam 최적화

- 모멘텀 최적화(지난 그레이디언트의 지수감소평균)와 RMSProp(지난 그레이디언트 제곱의 지수감소된 평균)의 아이디어를 합친 것.
- ![img](https://postfiles.pstatic.net/MjAxOTA4MTVfMzcg/MDAxNTY1ODgwMDgxMzM2.7ifxbUtf0uXVBWIzTQ9gJy5xkLrmGN0CWY_dl6dpf5Qg.H6rAa1XDe63gv15fkw7oNzoajPFb0wd2hwqg8-hjkgcg.PNG.jesusss91/image.png?type=w966)
- 모멘텀을 위한 하이퍼파라미터는 0.9, RMSProp의 하이퍼파라미터는 0.999를 사용하며 이는 보통 바꿀일 없이 학습률만을 튜닝하는 방식으로 사용한다.
- Adamax
  - Adam보다 안정적이지만 데이터 셋에 따라 다르며 보통 성능이 Adam이 더 나음
  - Adam이 동작하지 않으면 대신 시도해 볼만한 수준
- Nadam
  - adam + 네스테로프 기법
  - Adam보다 빠르게 수렴할 수 있음.
  - Ndam을 소개한 리포트에 의하면 일반적으로 nadam > adam이지만 RMSProp이 더 나을 떄도 있었다.

- ![img](https://k.kakaocdn.net/dn/cJEJUq/btqBtLNYJ1i/3Ka8dfUSzu1POPRaOMx4J0/img.png)



### 11.3.6 학습률 스케줄링

- 여러 학습률에 따라 지수적으로 학습률을 바꿔가며 좋은 값을 찾아야 한다.
- 가장 최적의 방법은 큰 학습률로 시작하여 학습속도가 느려질 수록 학습률을 줄이는 것.
  - 거듭제곱 기반 스케줄링
    - 학습률/(1+t/s)^c
    - 점점 느리게 감소
  - 지수 기반 스케줄링
    - 학습률*0.1^t/s
    - 계속 10배씩 감소
  - 구간별 고정 스케줄링
    - 일정기간 동안 학습후 그다음 에포크 동안 작은 학습률
  - 성능 기반 스케줄링
    - 매 N 스텝마다 오차 측정하고 오차가 줄어들지 않으면 몇 배씩 감소
  - 1사이클 스케줄링
    - 훈련 절반 동안 학습률을 선형적으로 증가하고 다시 나머지 절반동안 선형적으로 원상복귀
    - 마지막 몇번의 에포크는 학습률을 소수점 몇 째 자리까지 줄인다.(선형적으로)

