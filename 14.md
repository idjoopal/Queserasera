## 14. CNN 컴퓨터비전

- 시각피질 안의 많은 뉴런이 작은 국부수용장(시각의 어느부분을 인식하는 영역)을 가진 것으로 보임

  - ![핸즈온 머신러닝] 14장(1) - 합성곱 신경망을 사용한 컴퓨터 비전](https://blog.kakaocdn.net/dn/bfJK8K/btq0ApzSlkC/d2DqX7IKEnYr7CYRLsSgrk/img.png)

- 국부수용장 모델을 모방한 것이 CNN

- 합성곱 신경망 (CNN)

  - 합성곱층(Convolution layer)
  - 풀링층(Pooling Layer)

  ### 합성곱 층

  - CNN의 가장 중요한 요소이며 국부수용장을 모방함

  - 제로패딩 : 테두리(보통 0)를 추가하여 출력 층이 작아지는 것을 방지함.

  - 스트라이드(Stride)

    - <img src="https://seol8118.github.io/assets/images/book/3minDL/ch07-04-stride.jpg" alt="7장. 이미지 인식의 은총알 CNN - Seol&#39;s Blog" style="zoom:20%;" />
    - 수용장 사이에 간격을 두어 더 작은 층에 연결하는 방식
      - 모델의 계산복잡도를 낮춘다.

  - 필터(합성곱 커널)
    - 입력뉴런에 사용될 가중치 역할
    - 다양한 필터를 지정할 수 있으며 필터의 수를 하이퍼파라미터로 지정

  - 특성지도(Feature map)
    - 필터 각각을 사용하여 생성된 출력값
    - 수백개의 필터를 사용함
    - 필터에 포함된 뉴런은 동일 가중치와 편향
    - 필터별 가중치, 편향은 다름
    - 컬러채널(Color Channel)
      - 이미지를 대상으로하면 컬러의 경우 R,G,B 세개의 채널로 표현함
    - 뉴런의 출력값 : 입력에 대한 가중치의 합에 편향을 더함.

    $$
    z_{i,j,k} = b_{k} + \sum_{u=0}^{f_{h}-1}\sum_{v=0}^{f_{w}-1}\sum_{k=0}^{f_{n}-1}x_{i', j', k'} \times w_{u, v, k', k}
    $$

  ###  풀링 층(pooling layer)

  - 계산량과 메모리 사용량을 줄이면서, 과대적합의 위험도를 줄이는 용도.
  - 가중치를 사용하진 않지만 Stride를 사용하여 차원을 축소시킴
  - 최대 풀링층(Max pooling layer)
    - 2X2의 풀링라면.. : 4칸 중 가장 큰 값만 전달함
    - 파라미터 수를 획기적으로 줄일 수 있다.
    - 정보가 많이 사라짐
    - 작은 변화에 대한 불변성이 존재
      - <img src="https://blog.kakaocdn.net/dn/bely80/btqSgNivwAy/DKmBigEmaKg8jMURL5HKJ1/img.png" alt="Convolutional Neural Network(CNN)" style="zoom:50%;" />
  - 평균 풀링층(Average pooling layer)
    - 2X2의 풀링라면.. : 4칸의 평균값 활용
    - 최대 풀링층에 비해 성능은 떨어지지만 많은 정보가 보존됨
    - 전역 평균 풀링층
      - 각 Feature map의 평균을 계산하여 각 map에 대해 하나의 숫자를 출력
      - 현대 신경망 구조에서 종종 활용
  - 깊이별 최대/평균 풀링층
    - 각 Feature map에 대해 지정된 수만큼 만 최대/평균을 계산함
    - Feature map의 수가 줄어든다. 풀링커널 크기를 1로 지정하여 Feature map의 크기는 유지됨.
    - 다양한 특성에 대한 불변성 학습
      - 다양한 손글씨 인식하기.

### CNN 구조

![Kakao Brain](http://t1.kakaocdn.net/braincloud/homepage/article_image/201803220719161059350.png)



- LeNet-5 (CNN의 조상님 라스트팡)
  - 1998년에 소개됨
  - ![img](https://t1.daumcdn.net/cfile/tistory/99170D4C5C7E21250E)
  - 구조
    - 입력층
    - 3개의 Convolution layer(활성화함수 tanh)
    - 2개의 Subsampling layer - 평균 풀링 방식(활성화함수 tanh)
    - 1개의 완전연결 layer(활성화함수 tanh)
    - 출력층(활성화함수 RBF)
    - 입력 C1 S2 C3 S4 C5 F6 출력
- AlexNet
  - 2012 ILSVRC 우승 모델
  - ![img](https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMTA0MTRfMjc4%2FMDAxNjE4MzgyODE2NzEz.KvXt_isNz8Ro7AEuWAGapSESesb3HRjkwCPPzKY4G_Ag.GVidfoVq6m-8CCXyEbFGVmM6ugkmsrBpcd8GvHDRyr4g.PNG.falcon940105%2Fimage.png&type=sc960_832)
  - LeNet과 비슷하며 더 크고 깊은 구조.
    - 입력층
    - 5개의 Convolution layer(활성화함수 ReLU)
    - 2개의 Subsampling layer - 최대 풀링 방식(활성화함수 ReLU)
    - 2개의 완전연결 layer(활성화함수 ReLU)
    - 출력층(활성화함수 Softmax)
    - 입력 C1 S2 C3 S4 C5 C6 C7 F8 F9 F10 출력
  - 특징
    - 기존의 풀링은 보통 겹치지 않게 진행, AlexNet은 겹치는 영역을 만들어 과대적합을 방지함
    - 마지막 완전연결 F9, F10층에서 드롭아웃 50%를 사용
    - 데이터 증식사용
      - 훈련샘플을 여러가지 형태로 변형하여 인공생성
    - LRN(Local response normalization) 정규화 사용
      - 뉴런의 출력값을 경쟁적으로 함
      - 어떤 출력값이 큰 뉴런이 다른 특성화맵의 동일한 위치에 있는 뉴런을 억제함
- GoogLeNet
  - 2014년 ILSVRC 우승 모델
  - ![img](https://postfiles.pstatic.net/MjAyMDAzMjVfMjQ5/MDAxNTg1MTMzMTg5MjM5.bINAQG1QvnmoBGEBzS7pP8nr-rZ_0c4-r1dhy81T75Mg.1ryOdcGtqLfc3FYWI-isdxoR4rY3-J-1Ds3lH7Opc0Ig.PNG.nm1lee/image.png?type=w773)
  - 구조는 더 깊으나 파라미터는 AlexNet보다 10배 적음
  - 인셉션 모듈이라는 서브네트워크 활용
    - 여러 크기의 복잡한 패턴이 담긴 특성 맵을 출력함.
  - 1X1 컨벌루젼 사용
    - 깊이별 패턴을 확인하고, 다른 합성곱 층과 연계하는 역할을 수행한다.
    - 3X3과 5X5만 쓰기엔 연산량이 너무 많아서 1X1로 연산량을 줄여준다.
    - ![img](https://postfiles.pstatic.net/MjAyMDAzMjVfMjcz/MDAxNTg1MTMzMDY0OTUy.9_lZmxc3RguYAHocS7NStciNj-sVjvHOKX2TSQrPdt0g.jZM2R7pDZtcQIWfiO3lmpHQt-D_y247b_Blmown3jq4g.PNG.nm1lee/image.png?type=w773)
    - 깊이 연결에서 4층의 결과를 쌓음
  - ![img](https://postfiles.pstatic.net/MjAyMDA4MjBfNDMg/MDAxNTk3ODg4MzU3NzE4.7n2rVwXA_37fmC1W4gtJPYyDa2G19Q_q-XjryoSJrNgg.6WOXybuzYTPds-G9589CDPZeUVWN6h3pzB1KhPX2fR8g.PNG.another0430/image.png?type=w773)
    - 9개의 인셉션 모듈을 포함하고 있는 구조.
    - 모든 Convolution 층은 ReLU 활성화 함수를 사용한다.
- VGGNet
  - 2014년 ILSVRC 준우승 모델
  - 2~3개의 Convolution 층과 풀링층의 단순 반복
  - 총 16~19개의 Convolution 층 사용
  - 마지막에 2개의 은닉층과 출력층으로 구성된 밀집층 사용
  - 다수의 3X3 커널 필터 사용

